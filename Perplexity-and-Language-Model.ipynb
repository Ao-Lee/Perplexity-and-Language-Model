{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571eb4f9",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "suppose we have a small language model that can only output 3 words: 'hello', 'world' and 'today'. We give the first word of a sentence 'hello' and ask the language model to complete the second word. and our language model gives us:\n",
    "\n",
    "P('hello'|'hello')=0.1\n",
    "\n",
    "P('hello'|'world')=0.7\n",
    "\n",
    "P('hello'|'today')=0.2\n",
    "\n",
    "the entropy of this distribution will be \n",
    "$$\n",
    "-0.1*log(0.1)-0.7*log(0.7)-0.2*log(0.2)=0.802\n",
    "$$\n",
    "given a vocabulary set X, and a distribution P over this vocabulary set, the entropy of distribution P is:\n",
    "$$\n",
    "H(P(x)) =\\mathbb{E}_{P(x)}[-logP(x)] = -\\sum_{x}P(x)logP(x)\n",
    "$$\n",
    "where x represent  each word in this vocabulary. Entropy measures how complicated the distribution is. The more complicated the underlying distribution is, the higher the entropy will be.\n",
    "\n",
    "the smallest possible entropy for any distribution is zero. \n",
    "\n",
    "the entropy of a probability distribution is maximized when it is uniform. the language that has the maximal entropy is the one in which all the symbols appear with equal probability.\n",
    "\n",
    "Let |V| be the vocabulary size of an arbitrary language with the distribution P.\n",
    "$$\n",
    "H(P)\\le -|V|*\\frac{1}{|V|}*log(\\frac{1}{|V|})=log(|V|)\n",
    "$$\n",
    "\n",
    "### Cross Entropy and KL divergence \n",
    "\n",
    "suppose we are training a small language model that can only output 3 words: 'hello', 'world' and 'today'. Imaging the batch size is 1, and the sentence of the current training data is 'hello world'. We feed the first word 'hello' as input to the language model and ask the model to complete the second word. and our language model gives us a prediction distribution Q:\n",
    "\n",
    "Q('hello'|'hello')=0.1\n",
    "\n",
    "Q('world'|'hello')=0.7\n",
    "\n",
    "Q('today'|'hello')=0.2\n",
    "\n",
    "according to the input sentence, the ground truth distribution of the second word P is\n",
    "\n",
    "P('hello'|'hello')=0\n",
    "\n",
    "P('world'|'hello')=1\n",
    "\n",
    "P('today'|'hello')=0\n",
    "\n",
    "the cross entropy of the second word given two distribution P and Q is:  \n",
    "$$\n",
    "-0.0*log(0.1)-1*log(0.7)-0*log(0.2)=-log(0.7)=0.36\n",
    "$$\n",
    "given a vocabulary set X, and two distributions P and Q over this vocabulary set. you can imagine P is the empirical distribution of a given dataset, Q is the distribution of your model prediction. the cross entropy is defined as:\n",
    "$$\n",
    "H(P,Q) =\\mathbb{E}_{P(x)}[-logQ(x)] = -\\sum_{x}P(x)logQ(x)\n",
    "$$\n",
    "cross entropy measures how much the mismatch is between P and Q, the bigger the mismatch, the higher the cross entropy.\n",
    "\n",
    "why P is empirical distribution? because the true distribution of a language is unknown or hard to compute. In the above example, imagine you want to calculate the real probability of P(world|hello) over a large corpus with 1 trillion words and 1 million vocabulary, you need to search any bi-gram that start with 'hello' in this corpus, which is almost impossible. Empirical distribution P means that you assume the current training sentence is the whole corpus and the ground truth distribution is a one hot vector. If the ground truth of current word is w, cross entropy can be simplified as \n",
    "$$\n",
    "H = -log(p(w))\n",
    "$$\n",
    "if we want to measure how different Q is from P, then cross entropy may not be a good measurement. why? if H(P,Q) is high, there are two possible reasons:\n",
    "\n",
    "* reason 1: distribution Q is very different from P, this is exactly what we want to measure\n",
    "* reason 2:  the underlying distribution P is complicated, namely H(P) is high, this is because H(P) is the lower bound of H(P,Q). Later, we will proof that $H(P,Q)\\ge H(P)$\n",
    "\n",
    "to measure reason 1 rather than reason 2, we can 'normalize' the cross entropy with $H(P,Q)-H(P)$\n",
    "\n",
    "the KL divergence is defined as:\n",
    "$$\n",
    "D_{KL}(P||Q) = H(P,Q)-H(P) = \\sum_x P(x)log\\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "we notice that P is ground truth distribution, so H(P) is constant and can not be optimized. When optimizing cross entropy H(P, Q) using gradient descent , it is equivalent to optimize KL divergence as well.\n",
    "\n",
    "since $D_{KL}(P||Q)\\ge0$ (we will proof that later),  $H(P,Q)\\ge H(P)$, and so H(P) is the lower bound of H(P,Q)\n",
    "\n",
    "the minimal KL divergence is 0 when P and Q are the same distribution:\n",
    "$$\n",
    "D_{KL}(P||Q) =\\sum_x P(x)log\\frac{P(x)}{Q(x)} = \\sum_x P(x)log1=0\n",
    "$$\n",
    "\n",
    "### KL divergence is always non negative\n",
    "\n",
    "note that: $log(a)\\le a-1$ for all a > 0 \n",
    "$$\n",
    "-D(P,Q)= -\\sum_x P(x)log\\frac{P(x)}{Q(x)}\n",
    "=\\sum_x P(x)log\\frac{Q(x)}{P(x)}\\le\\sum_x P(x)(\\frac{Q(x)}{P(x)}-1)=\\sum_x P(x)-\\sum_x Q(x)=1-1=0\n",
    "$$\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "perplexity is defined as:\n",
    "$$\n",
    "Perplexity(P,Q) = 2^{H(P,Q)}\n",
    "$$\n",
    "entropy uses logarithms, well perplexity, with its exponentiation bring it back to a linear scale. Which we, human, usually prefer.\n",
    "\n",
    "### Cross Entropy loss Vs. Sentence Perplexity\n",
    "\n",
    "during training, rather than computing cross entropy of a particular word, one wants to sum the cross entropy for each word in the given sentence and take the average. suppose we have k words in the input sentence $[w_1,w_2,...,w_k]$, the average cross entropy loss for the whole sentence is\n",
    "$$\n",
    "loss = -\\frac{1}{k}\\sum_klog(p(w_k))\n",
    "$$\n",
    "the average sentence perplexity is\n",
    "$$\n",
    "2^{-\\frac{1}{k}\\sum_klog(p(w_k))}= (\\prod_{k}p(w_k))^{-\\frac{1}{k}}\n",
    "$$\n",
    "the lower the perplexity over a well-written sentence, the better is the language model.\n",
    "\n",
    "### Another interpretation: normalized sentence probability \n",
    "\n",
    "suppose we have trained a small language model over an English corpus. The model is only able to predict the probability of the next word in the sentence from a small subset of six words: *“a”*, *“the”*, *“red”*, *“fox”*, *“dog”*, and *“.”*. Let’s compute the probability of the sentence *W,* which is “*a red fox.*”.\n",
    "\n",
    "P(“a red fox.”) = P(“a”) * P(“red” | “a”) * P(“fox” | “a red”) * P(“.” | “a red fox”)\n",
    "\n",
    "Formally, A language model assigns probabilities to sequences of arbitrary symbols such that the more likely a sequence $(w_1,w_2,...w_k)$is to exist in that language, the higher the probability.  Given a language model and a sentence containing n words, the probability of a sentence is given by\n",
    "$$\n",
    "P(w_1, w_2,...w_k) = p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)...p(w_k|w_1,w_2,...,w_{k-1})\n",
    "= \\prod_{i}^{k}p(w_i|w_1,...w_{i-1})\n",
    "$$\n",
    "It would be nice to compare the probabilities assigned to different sentences to see which sentences are better predicted by the language model. However, since the probability of a sentence is obtained from a product of probabilities, the longer is the sentence the lower will be its probability, since it’s a product of factors with values smaller than one. \n",
    "\n",
    "We should find a way of measuring these sentence probabilities, without the influence of the sentence length. This can be done by normalizing the sentence probability by the number of words in the sentence. Since the probability of a sentence is obtained by multiplying many factors, we can average them using the [geometric mean](https://en.wikipedia.org/wiki/Geometric_mean).\n",
    "$$\n",
    "P_{norm}(W) =P_{norm}(w_1, w_2,...w_k) = P(w_1, w_2,...w_k)^{1/k}\n",
    "$$\n",
    "in such way, sentence perplexity is just the reciprocal of the normalized sentence probability.\n",
    "$$\n",
    "Perplexity(W) = 1/P_{norm}(W) =P(w_1, w_2,...w_k)^{-1/k} = (\\prod_{k}p(w_k))^{-\\frac{1}{k}}\n",
    "$$\n",
    "\n",
    "### perplexity is hard to compute in practice\n",
    "\n",
    "you can get wildly different result depending on the details:\n",
    "\n",
    "how to create token: words? bytes? characters?\n",
    "\n",
    "how to handle tokens that are out-of-vocabulary: ignore? using special [unknown] token?\n",
    "\n",
    "what base of Logarithm to use? normally 2, but other people may use pi, 10, or e\n",
    "\n",
    "how to define 'average perplexity'? take the average by sentence? by paragraph? or you treat the whole evaluation dataset as a long sentence?\n",
    "\n",
    "feed the model with a special [start] token or not\n",
    "\n",
    "if input sentences are too long for the model, what do you do with them: throw them out? truncation?\n",
    "\n",
    "### references\n",
    "\n",
    "https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584\n",
    "\n",
    "https://thegradient.pub/understanding-evaluation-metrics-for-language-models/\n",
    "\n",
    "https://www.youtube.com/watch?v=kdaX9p6Uc9k\n",
    "\n",
    "https://stats.stackexchange.com/questions/335197/why-kl-divergence-is-non-negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc32d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
